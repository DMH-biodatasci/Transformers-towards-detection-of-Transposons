{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69058e57",
   "metadata": {},
   "source": [
    "## Load and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9ea1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TE_lib as te\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5bb5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of the two lines of code below\n",
    "\n",
    "#df, tok = te.prep_tvt_from_func_1()  # compute data from original files\n",
    "df, tok = te.read_tvt()               # read data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb72682d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>chunk</th>\n",
       "      <th>set</th>\n",
       "      <th>tokens</th>\n",
       "      <th>token_ids</th>\n",
       "      <th>attention_masks</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.fasta</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>[mcl00578, mcl00294, mcl01096, mcl01000, mcl01...</td>\n",
       "      <td>[11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.fasta</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>[mcl01236, mcl00376, mcl04030, mcl00368, mcl01...</td>\n",
       "      <td>[61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 7...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.fasta</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>[mcl00784, mcl01263, mcl00374, mcl01238, mcl00...</td>\n",
       "      <td>[111, 112, 113, 114, 115, 116, 117, 118, 119, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.fasta</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>[mcl03193, mcl02304, mcl00809, mcl01966, mcl02...</td>\n",
       "      <td>[160, 161, 162, 163, 164, 165, 166, 167, 168, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.fasta</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>[mcl02582, mcl03806, mcl03733, mcl03779, mcl03...</td>\n",
       "      <td>[210, 211, 212, 213, 214, 215, 216, 217, 218, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230222</th>\n",
       "      <td>9998.fasta</td>\n",
       "      <td>85</td>\n",
       "      <td>training</td>\n",
       "      <td>[mcl01916, mcl00745, mcl06673, mcl06098, mcl00...</td>\n",
       "      <td>[3154, 3153, 6115, 6116, 3150, 3145, 3144, 314...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230223</th>\n",
       "      <td>9998.fasta</td>\n",
       "      <td>86</td>\n",
       "      <td>training</td>\n",
       "      <td>[mcl04304, mcl04602, mcl05925, mcl12035, mcl12...</td>\n",
       "      <td>[6139, 6140, 3116, 6141, 6142, 6143, 6144, 614...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230224</th>\n",
       "      <td>9999.fasta</td>\n",
       "      <td>1</td>\n",
       "      <td>training</td>\n",
       "      <td>[mcl09252, mcl09004, mcl11223, mcl09170, mcl02...</td>\n",
       "      <td>[6181, 6182, 6183, 6184, 3445, 3444, 6185, 618...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230225</th>\n",
       "      <td>9999.fasta</td>\n",
       "      <td>2</td>\n",
       "      <td>training</td>\n",
       "      <td>[mcl07933, mcl08867, mcl07944, mcl08857, mcl07...</td>\n",
       "      <td>[6227, 6228, 6229, 6230, 6231, 6232, 6233, 623...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230226</th>\n",
       "      <td>9999.fasta</td>\n",
       "      <td>3</td>\n",
       "      <td>training</td>\n",
       "      <td>[mcl08832, mcl08783, mcl08981, mcl07062, mcl00...</td>\n",
       "      <td>[6276, 6277, 6278, 6279, 2524, 4174, 6280, 628...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230227 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            origin  chunk       set  \\\n",
       "0        100.fasta      1      test   \n",
       "1        100.fasta      2      test   \n",
       "2        100.fasta      3      test   \n",
       "3        100.fasta      4      test   \n",
       "4        100.fasta      5      test   \n",
       "...            ...    ...       ...   \n",
       "230222  9998.fasta     85  training   \n",
       "230223  9998.fasta     86  training   \n",
       "230224  9999.fasta      1  training   \n",
       "230225  9999.fasta      2  training   \n",
       "230226  9999.fasta      3  training   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [mcl00578, mcl00294, mcl01096, mcl01000, mcl01...   \n",
       "1       [mcl01236, mcl00376, mcl04030, mcl00368, mcl01...   \n",
       "2       [mcl00784, mcl01263, mcl00374, mcl01238, mcl00...   \n",
       "3       [mcl03193, mcl02304, mcl00809, mcl01966, mcl02...   \n",
       "4       [mcl02582, mcl03806, mcl03733, mcl03779, mcl03...   \n",
       "...                                                   ...   \n",
       "230222  [mcl01916, mcl00745, mcl06673, mcl06098, mcl00...   \n",
       "230223  [mcl04304, mcl04602, mcl05925, mcl12035, mcl12...   \n",
       "230224  [mcl09252, mcl09004, mcl11223, mcl09170, mcl02...   \n",
       "230225  [mcl07933, mcl08867, mcl07944, mcl08857, mcl07...   \n",
       "230226  [mcl08832, mcl08783, mcl08981, mcl07062, mcl00...   \n",
       "\n",
       "                                                token_ids  \\\n",
       "0       [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2...   \n",
       "1       [61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 7...   \n",
       "2       [111, 112, 113, 114, 115, 116, 117, 118, 119, ...   \n",
       "3       [160, 161, 162, 163, 164, 165, 166, 167, 168, ...   \n",
       "4       [210, 211, 212, 213, 214, 215, 216, 217, 218, ...   \n",
       "...                                                   ...   \n",
       "230222  [3154, 3153, 6115, 6116, 3150, 3145, 3144, 314...   \n",
       "230223  [6139, 6140, 3116, 6141, 6142, 6143, 6144, 614...   \n",
       "230224  [6181, 6182, 6183, 6184, 3445, 3444, 6185, 618...   \n",
       "230225  [6227, 6228, 6229, 6230, 6231, 6232, 6233, 623...   \n",
       "230226  [6276, 6277, 6278, 6279, 2524, 4174, 6280, 628...   \n",
       "\n",
       "                                          attention_masks  \\\n",
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                   ...   \n",
       "230222  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "230223  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "230224  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "230225  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "230226  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                                   labels  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "230222  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "230223  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "230224  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "230225  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "230226  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...  \n",
       "\n",
       "[230227 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e14ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique tokens in the data:  32634\n"
     ]
    }
   ],
   "source": [
    "print(\"number of unique tokens in the data: \", len(tok.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c59290ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of each chunk in the data:  150\n"
     ]
    }
   ],
   "source": [
    "print(\"length of each chunk in the data: \", len(df[\"token_ids\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9132ee",
   "metadata": {},
   "source": [
    "## Build masked training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f610b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_id = 1 # to be checked if the model requires another pre-defined token-id\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 11] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    #labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    #labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    encoded_texts_masked[inp_mask] = mask_token_id\n",
    "    \n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b61940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df[df[\"set\"] == \"training\"][\"token_ids\"]\n",
    "df_validation = df[df[\"set\"] == \"validation\"][\"token_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46f7c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_masked, training_labels = get_masked_input_and_labels(np.array([x for x in df_training.tolist()]))\n",
    "validation_masked, validation_labels = get_masked_input_and_labels(np.array([x for x in df_validation.tolist()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8f44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_attention_mask = np.array([x for x in df[df[\"set\"] == \"training\"][\"attention_masks\"]])\n",
    "validation_attention_mask = np.array([x for x in df[df[\"set\"] == \"validation\"][\"attention_masks\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6739ab23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 440, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855,\n",
       "       4856, 4857, 4858, 4859, 4860,    1, 4862,    1, 4864,    1, 4866,\n",
       "          1, 4868, 4869,    1, 4871, 4872, 4873, 4874,    1, 4876, 4877,\n",
       "       4878, 4879, 4880, 4881, 4882, 4883, 4884,    1, 1210,    1, 4887,\n",
       "       4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 2785,    1,  407,\n",
       "       4897, 4898,    1, 4900, 2943, 4901,    1, 4902,  373,  374, 4903,\n",
       "       4904, 4905, 4906, 4907,    1,    1, 4910, 4911, 4912, 4913, 4914,\n",
       "       4915, 4916, 4917, 4918, 4919, 4920,    1, 4922, 4923,    1, 4925,\n",
       "          1, 4927,    1, 4929,    1, 4931, 4932, 2137, 4933, 4934, 1878,\n",
       "       4935, 4936, 4937, 4938, 4939, 4940,    1, 4942, 4943, 4944, 4945,\n",
       "       4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956,\n",
       "          1, 4958,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_masked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fe78183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 440, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855,\n",
       "       4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866,\n",
       "       4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877,\n",
       "       4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 1210, 4886, 4887,\n",
       "       4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 2785, 4896,  407,\n",
       "       4897, 4898, 4899, 4900, 2943, 4901, 4637, 4902,  373,  374, 4903,\n",
       "       4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914,\n",
       "       4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925,\n",
       "       4926, 4927, 4928, 4929, 4930, 4931, 4932, 2137, 4933, 4934, 1878,\n",
       "       4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945,\n",
       "       4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956,\n",
       "       4957, 4958,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a472e29f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f432f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3551, 3552, 3553,    1, 5903, 5904, 5905, 5906, 3557, 3558, 3559,\n",
       "       3560, 3561, 1940, 1904, 1903, 1902, 1901, 1900, 3912,  472,  473,\n",
       "        474,  475,  476,  477,    1,  479, 3563, 3564, 3565, 3566, 3567,\n",
       "       7347, 3574, 3575, 3576,  247, 5910, 5911, 5912, 3577, 6372, 3578,\n",
       "       3579, 3580, 3581,    1, 2093,    1, 2091, 3582, 3583, 3584, 3585,\n",
       "       3588, 5913, 6973, 5914, 5915, 5916, 6974,    1, 5918, 6975, 3589,\n",
       "       5919, 3590, 4509, 4510, 4511, 4512, 4513, 4514,    1, 5921,    1,\n",
       "       5922, 5923, 5924,    1, 2062, 5925,    1, 4616, 6370, 4618,    1,\n",
       "       6978, 5929, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602,\n",
       "       3603, 3604,    1, 3606, 5930, 5931, 3608, 3609, 3610, 3611, 3612,\n",
       "       3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 5934,\n",
       "       3623, 3625, 3624,    1,    1, 3627, 3628,    1, 3630, 3632, 5936,\n",
       "       3633, 3634,    1, 3636,    1, 3638, 3639, 3640, 3641, 6979, 5937,\n",
       "       5938, 3642, 3643,    1,    1, 3646, 3647])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_masked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a6eea4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3551, 3552, 3553, ..., 3645, 3646, 3647],\n",
       "       [2091, 3582, 3583, ..., 3701, 3702, 3704],\n",
       "       [3604, 3605, 3606, ..., 3936, 3937, 3938],\n",
       "       ...,\n",
       "       [3208, 3207, 3206, ..., 3063, 3062, 6165],\n",
       "       [3158, 5084, 5083, ..., 2999, 2998, 2997],\n",
       "       [6138, 3123, 3122, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a10cb4",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "    freshly initialize a DistilBERT model. We’ll use the same configuration for our model as for the distilbert-base-uncased model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and max length of embeddings matches length of our chunks. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41eb11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForMaskedLM, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    vocab_size=len(tok.keys())+10,   # +10 for the reserved special tokens\n",
    "    max_position_embeddings=len(df[\"token_ids\"].iloc[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d0c01",
   "metadata": {},
   "source": [
    "## Load a new (not pretrained) model\n",
    "Load a new model. Note that we don’t use the from_pretrained() function, since we’re actually initializing a model ourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "971aef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_masked_lm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 67714560  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " vocab_transform (Dense)     multiple                  590592    \n",
      "                                                                 \n",
      " vocab_layer_norm (LayerNorm  multiple                 1536      \n",
      " alization)                                                      \n",
      "                                                                 \n",
      " vocab_projector (TFDistilBe  multiple                 25219972  \n",
      " rtLMHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,339,332\n",
      "Trainable params: 68,339,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForMaskedLM(config)\n",
    "model(model.dummy_inputs)  # Builds the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2e1763",
   "metadata": {},
   "source": [
    "## Log in to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e676dd",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45877776",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "configure the training hyperparameters and call compile() and fit(). We’ll use a learning rate schedule with some warmup to improve the stability of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aabd6b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# with tensorflow\n",
    "\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(training_masked[0:100])   ## change this when training with the full data\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=5e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3305e1f",
   "metadata": {},
   "source": [
    "### Convert the data dictionaries\n",
    "\n",
    "\n",
    "    there are three possibilities you can use to gather all the input Tensors in the first positional argument :\n",
    "\n",
    "    a single Tensor with input_ids only and nothing else: model(inputs_ids)\n",
    "    a list of varying length with one or several input Tensors IN THE ORDER given in the docstring: model([input_ids, attention_mask])\n",
    "    a dictionary with one or several input Tensors associated to the input names given in the docstring: model({\"input_ids\": input_ids})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eef3f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data has been reduced to first 100 rows for training and 20 rows for validation.\n",
    "# This should be changed to the full data as soon as we see the model.fit running without bugs\n",
    "\n",
    "tf_train_dict = { 'input_ids': tf.convert_to_tensor(training_masked[0:100]),\n",
    "           'attention_mask': tf.convert_to_tensor(training_attention_mask[0:100]),\n",
    "           'labels': tf.convert_to_tensor(training_labels[0:100])\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49525cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_val_dict = { 'input_ids': tf.convert_to_tensor(validation_masked[0:20]),\n",
    "           'attention_mask': tf.convert_to_tensor(validation_attention_mask[0:20]),\n",
    "           'labels': tf.convert_to_tensor(validation_labels[0:20])\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b599b8a",
   "metadata": {},
   "source": [
    "## Train the model with reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa4ef54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_training_masked = training_masked[0: 10]\n",
    "short_training_attention_mask = training_attention_mask[0: 10]\n",
    "short_training_labels = training_labels[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e336d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 3\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce550a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch  1\n",
      "1/1 [==============================] - 12s 12s/step - loss: 10.5446 - val_loss: 10.5024\n",
      "1/1 [==============================] - 9s 9s/step - loss: 10.4866 - val_loss: 10.5022\n",
      "1/1 [==============================] - 9s 9s/step - loss: 10.4932 - val_loss: 10.5019\n",
      "1/1 [==============================] - 9s 9s/step - loss: 10.4922 - val_loss: 10.5016\n",
      "start epoch  2\n",
      "1/1 [==============================] - 11s 11s/step - loss: 10.5426 - val_loss: 10.5013\n",
      "1/1 [==============================] - 11s 11s/step - loss: 10.4857 - val_loss: 10.5010\n",
      "1/1 [==============================] - 12s 12s/step - loss: 10.4885 - val_loss: 10.5006\n",
      "1/1 [==============================] - 10s 10s/step - loss: 10.4858 - val_loss: 10.5002\n",
      "--------------------------- end ---------------------------------\n"
     ]
    }
   ],
   "source": [
    "#from transformers.keras_callbacks import PushToHubCallback\n",
    "#callback = PushToHubCallback(output_dir=\"codeparrot-ds\", tokenizer=tokenizer)\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    print(\"start epoch \", epoch+1)\n",
    "    for start in range(0, len(short_training_masked), mini_batch_size):\n",
    "        end = min(start + mini_batch_size, len(short_training_masked)) \n",
    "        mini_batch = { 'input_ids': tf.convert_to_tensor(short_training_masked[start: end]),\n",
    "                      'attention_mask': tf.convert_to_tensor(short_training_attention_mask[start: end]),\n",
    "                      'labels': tf.convert_to_tensor(short_training_labels[start: end])\n",
    "         }\n",
    "\n",
    "        model.fit(mini_batch, validation_data=tf_val_dict, epochs=1, batch_size = mini_batch_size, verbose=1) #, callbacks=[callback])\n",
    "\n",
    "print(\"--------------------------- end ---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81df8e",
   "metadata": {},
   "source": [
    "## Train the model with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d45693f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fafe733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch  1\n",
      "1/1 [==============================] - 92s 92s/step - loss: 10.5085 - val_loss: 10.4998\n",
      "1/1 [==============================] - 97s 97s/step - loss: 10.5154 - val_loss: 10.4992\n",
      "1/1 [==============================] - 97s 97s/step - loss: 10.5131 - val_loss: 10.4984\n",
      "1/1 [==============================] - 79s 79s/step - loss: 10.4957 - val_loss: 10.4975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21072/1696427697.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m          }\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_val_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#, callbacks=[callback])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------------------------- end ---------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from transformers.keras_callbacks import PushToHubCallback\n",
    "#callback = PushToHubCallback(output_dir=\"codeparrot-ds\", tokenizer=tokenizer)\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    print(\"start epoch \", epoch+1)\n",
    "    for start in range(0, len(training_masked), mini_batch_size):\n",
    "        end = min(start + mini_batch_size, len(training_masked)) \n",
    "        mini_batch = { 'input_ids': tf.convert_to_tensor(training_masked[start: end]),\n",
    "                      'attention_mask': tf.convert_to_tensor(training_attention_mask[start: end]),\n",
    "                      'labels': tf.convert_to_tensor(training_labels[start: end])\n",
    "         }\n",
    "\n",
    "        model.fit(mini_batch, validation_data=tf_val_dict, epochs=1, batch_size = mini_batch_size, verbose=1) #, callbacks=[callback])\n",
    "\n",
    "print(\"--------------------------- end ---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792279df",
   "metadata": {},
   "source": [
    "## Get an output from the newly pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f9cf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(i, predictions):\n",
    "    probs = tf.nn.softmax(predictions[0, i])\n",
    "    result = tf.math.top_k(probs, k=3)\n",
    "    pred = [result.indices.numpy(), result.values.numpy()]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af12869",
   "metadata": {},
   "source": [
    "#### Define input: Select a single chunk input from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29ef5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_element = 101\n",
    "input = validation_masked[test_element]\n",
    "target = validation_labels[test_element]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4e74d",
   "metadata": {},
   "source": [
    "#### Predict with the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfcf6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4d81d",
   "metadata": {},
   "source": [
    "#### Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "983e1e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 2302, 2301, 2300,    1, 2298, 2297, 5136, 3086, 5137, 5138,\n",
       "       5139, 5140, 6538, 5142, 6539, 5143, 7115, 5144, 2296, 2295,    1,\n",
       "       5145, 2289, 2288, 2287, 5146,    1,    1, 2284, 2283, 2279, 2278,\n",
       "       2277, 2276,    1,    1, 2273, 2272, 2271, 2270, 2269, 2268, 2267,\n",
       "       2266, 2265,    1,    1, 7113,    1, 7111, 7110, 6990, 4986,    1,\n",
       "       7109, 5147, 5148, 2263, 2262, 2261,    1, 2259, 5149, 2258, 2257,\n",
       "       2256, 2255,    1, 2253,    1, 2251, 2246, 5150,    1, 5152, 5153,\n",
       "       5154, 2212, 2211,    1, 2209, 2208, 2207,    1,    1, 7108, 2205,\n",
       "       2204, 2203, 2202, 2201, 2200, 2199, 5156, 7107, 5157, 2197, 2196,\n",
       "       2195,    1, 2193, 2192, 2191, 2190, 2189,    1, 2186,    1,    1,\n",
       "       2183, 2179,    1, 2177, 2176, 2175,    1, 2173,    1, 2170,    1,\n",
       "       2168, 2167, 2166, 2165, 2164, 2163, 2162, 5159, 2160, 2159, 2158,\n",
       "       2157, 2156, 2155, 2154, 2151, 2150, 7105,    1, 7103,    1, 7101,\n",
       "       7100, 7099,    1, 7097, 7096, 2148, 2147])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dacac2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2307, 2302, 2301, 2300, 2299, 2298, 2297, 5136, 3086, 5137, 5138,\n",
       "       5139, 5140, 6538, 5142, 6539, 5143, 7115, 5144, 2296, 2295, 2291,\n",
       "       5145, 2289, 2288, 2287, 5146, 2286, 2285, 2284, 2283, 2279, 2278,\n",
       "       2277, 2276, 2275, 2274, 2273, 2272, 2271, 2270, 2269, 2268, 2267,\n",
       "       2266, 2265, 2264, 7114, 7113, 7112, 7111, 7110, 6990, 4986, 4985,\n",
       "       7109, 5147, 5148, 2263, 2262, 2261, 2260, 2259, 5149, 2258, 2257,\n",
       "       2256, 2255, 2254, 2253, 2252, 2251, 2246, 5150, 5151, 5152, 5153,\n",
       "       5154, 2212, 2211, 2210, 2209, 2208, 2207, 2206, 5155, 7108, 2205,\n",
       "       2204, 2203, 2202, 2201, 2200, 2199, 5156, 7107, 5157, 2197, 2196,\n",
       "       2195, 2194, 2193, 2192, 2191, 2190, 2189, 5158, 2186, 2187, 2184,\n",
       "       2183, 2179, 2178, 2177, 2176, 2175, 2174, 2173, 2171, 2170, 2169,\n",
       "       2168, 2167, 2166, 2165, 2164, 2163, 2162, 5159, 2160, 2159, 2158,\n",
       "       2157, 2156, 2155, 2154, 2151, 2150, 7105, 7104, 7103, 7102, 7101,\n",
       "       7100, 7099, 7098, 7097, 7096, 2148, 2147])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "959164a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mask] at position:  0 \n",
      "\tprediction is: [27660  8948 25219] \n",
      "\tweight is: [0.00019314 0.00018957 0.00017921] \n",
      "\ttarget was:  2307\n",
      "[mask] at position:  4 \n",
      "\tprediction is: [10654 19123 25007] \n",
      "\tweight is: [0.00019097 0.00016114 0.00015968] \n",
      "\ttarget was:  2299\n",
      "[mask] at position:  21 \n",
      "\tprediction is: [27660    20 30564] \n",
      "\tweight is: [0.00018934 0.00018866 0.00016391] \n",
      "\ttarget was:  2291\n",
      "[mask] at position:  27 \n",
      "\tprediction is: [26252 23175 16251] \n",
      "\tweight is: [0.00019577 0.00018884 0.0001659 ] \n",
      "\ttarget was:  2286\n",
      "[mask] at position:  28 \n",
      "\tprediction is: [ 1441  9074 21146] \n",
      "\tweight is: [0.00019922 0.00019419 0.00017677] \n",
      "\ttarget was:  2285\n",
      "[mask] at position:  35 \n",
      "\tprediction is: [ 5532  6394 16893] \n",
      "\tweight is: [0.00017323 0.00016954 0.00015445] \n",
      "\ttarget was:  2275\n",
      "[mask] at position:  36 \n",
      "\tprediction is: [24433 17428 29763] \n",
      "\tweight is: [0.00021104 0.00018698 0.0001784 ] \n",
      "\ttarget was:  2274\n",
      "[mask] at position:  46 \n",
      "\tprediction is: [ 2910 14889 12568] \n",
      "\tweight is: [0.00022252 0.00019489 0.00018116] \n",
      "\ttarget was:  2264\n",
      "[mask] at position:  47 \n",
      "\tprediction is: [21610 17540  2628] \n",
      "\tweight is: [0.00024741 0.00021499 0.00018981] \n",
      "\ttarget was:  7114\n",
      "[mask] at position:  49 \n",
      "\tprediction is: [27660  2628  5124] \n",
      "\tweight is: [0.00023267 0.00018749 0.00018717] \n",
      "\ttarget was:  7112\n",
      "[mask] at position:  54 \n",
      "\tprediction is: [17428  3501 12817] \n",
      "\tweight is: [0.00026491 0.0001959  0.00017969] \n",
      "\ttarget was:  4985\n",
      "[mask] at position:  61 \n",
      "\tprediction is: [27660 15391 32245] \n",
      "\tweight is: [0.00019817 0.00017884 0.00016801] \n",
      "\ttarget was:  2260\n",
      "[mask] at position:  68 \n",
      "\tprediction is: [22926 14889  8582] \n",
      "\tweight is: [0.00021224 0.00020611 0.0001843 ] \n",
      "\ttarget was:  2254\n",
      "[mask] at position:  70 \n",
      "\tprediction is: [26252 22133 31991] \n",
      "\tweight is: [0.00022507 0.00020605 0.00018258] \n",
      "\ttarget was:  2252\n",
      "[mask] at position:  74 \n",
      "\tprediction is: [12553 17428 28288] \n",
      "\tweight is: [0.0001939  0.00018621 0.00016393] \n",
      "\ttarget was:  5151\n",
      "[mask] at position:  80 \n",
      "\tprediction is: [19065  5018  7682] \n",
      "\tweight is: [0.00018512 0.00018398 0.00017484] \n",
      "\ttarget was:  2210\n",
      "[mask] at position:  84 \n",
      "\tprediction is: [ 7682 10326  9138] \n",
      "\tweight is: [0.00028991 0.00017368 0.0001714 ] \n",
      "\ttarget was:  2206\n",
      "[mask] at position:  85 \n",
      "\tprediction is: [12712  4722  2681] \n",
      "\tweight is: [0.00018189 0.00016824 0.00016759] \n",
      "\ttarget was:  5155\n",
      "[mask] at position:  100 \n",
      "\tprediction is: [24433 30895  5621] \n",
      "\tweight is: [0.0002136  0.00019654 0.00019308] \n",
      "\ttarget was:  2194\n",
      "[mask] at position:  106 \n",
      "\tprediction is: [ 1395 23458  9961] \n",
      "\tweight is: [0.00017963 0.00017569 0.00017083] \n",
      "\ttarget was:  5158\n",
      "[mask] at position:  108 \n",
      "\tprediction is: [17540 25008 16096] \n",
      "\tweight is: [0.00022604 0.00018597 0.00016861] \n",
      "\ttarget was:  2187\n",
      "[mask] at position:  109 \n",
      "\tprediction is: [ 4184 19462 26509] \n",
      "\tweight is: [0.00020045 0.000184   0.00017475] \n",
      "\ttarget was:  2184\n",
      "[mask] at position:  112 \n",
      "\tprediction is: [ 2628 21076 31632] \n",
      "\tweight is: [0.00022531 0.00019769 0.00018005] \n",
      "\ttarget was:  2178\n",
      "[mask] at position:  116 \n",
      "\tprediction is: [ 9297 10899 11997] \n",
      "\tweight is: [0.00015581 0.00015526 0.00015388] \n",
      "\ttarget was:  2174\n",
      "[mask] at position:  118 \n",
      "\tprediction is: [16399 11861 21610] \n",
      "\tweight is: [0.0002201  0.00018213 0.0001549 ] \n",
      "\ttarget was:  2171\n",
      "[mask] at position:  120 \n",
      "\tprediction is: [ 7682 24946 28626] \n",
      "\tweight is: [0.00022625 0.00016865 0.00016503] \n",
      "\ttarget was:  2169\n",
      "[mask] at position:  139 \n",
      "\tprediction is: [24433  1785 17505] \n",
      "\tweight is: [0.00017714 0.00017563 0.00017351] \n",
      "\ttarget was:  7104\n",
      "[mask] at position:  141 \n",
      "\tprediction is: [30965 19462 19240] \n",
      "\tweight is: [0.00018015 0.00017408 0.00016767] \n",
      "\ttarget was:  7102\n",
      "[mask] at position:  145 \n",
      "\tprediction is: [22268 26481  1490] \n",
      "\tweight is: [0.00022124 0.0001828  0.00017701] \n",
      "\ttarget was:  7098\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x in input:\n",
    "    if x == 1:\n",
    "        print(\"[mask] at position: \", i, \n",
    "              \"\\n\\tprediction is:\", get_prediction(i, outputs[0])[0],\n",
    "              \"\\n\\tweight is:\", get_prediction(i, outputs[0])[1],\n",
    "              \"\\n\\ttarget was: \", target[i])\n",
    "    i = i+1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca07975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
